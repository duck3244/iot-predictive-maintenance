"""
XML ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ .dat íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ AI í•™ìŠµìš© ë°ì´í„°ë¡œ ë³€í™˜
"""

import os
import sys
import pandas as pd
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€ (ëª¨ë“ˆ importë¥¼ ìœ„í•´)
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

try:
    from xml_data_processor import XMLDataProcessor
    print("âœ… xml_data_processor ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ xml_data_processor ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("xml_data_processor.py íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ ë˜ëŠ” ìƒìœ„ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*70)
    print(" ğŸ¤– SCARA ë¡œë´‡ XML ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
    print("="*70)
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    data_dir = "data/xml_data"
    
    # ìƒëŒ€ ê²½ë¡œë¡œ ë°ì´í„° ë””ë ‰í† ë¦¬ ì°¾ê¸°
    possible_paths = [
        data_dir,
        os.path.join("..", data_dir),
        os.path.join(".", "xml_data"),
        "xml_data",
        "."  # í˜„ì¬ ë””ë ‰í† ë¦¬
    ]
    
    actual_data_dir = None
    for path in possible_paths:
        if os.path.exists(path):
            dat_files = [f for f in os.listdir(path) if f.endswith('.dat')]
            if dat_files:
                actual_data_dir = path
                print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ ë°œê²¬: {os.path.abspath(path)}")
                print(f"ğŸ“„ .dat íŒŒì¼ ìˆ˜: {len(dat_files)}ê°œ")
                break
    
    if not actual_data_dir:
        print("âŒ .dat íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ìœ„ì¹˜ ì¤‘ í•˜ë‚˜ì— .dat íŒŒì¼ì„ ë°°ì¹˜í•˜ì„¸ìš”:")
        for path in possible_paths:
            print(f"   - {os.path.abspath(path)}")
        return
    
    # XML ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    print(f"\nğŸ”§ XML ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”...")
    processor = XMLDataProcessor(actual_data_dir)
    
    # íŒŒì¼ ëª©ë¡ í™•ì¸
    dat_files = [f for f in os.listdir(actual_data_dir) if f.endswith('.dat')]
    print(f"ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ (ì²˜ìŒ 5ê°œ):")
    for i, filename in enumerate(dat_files[:5]):
        print(f"   {i+1}. {filename}")
    if len(dat_files) > 5:
        print(f"   ... ë° {len(dat_files)-5}ê°œ ì¶”ê°€ íŒŒì¼")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input(f"\nğŸš€ {len(dat_files)}ê°œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
    if response.lower() != 'y':
        print("ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    print(f"\nâš¡ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
    print("ì´ ê³¼ì •ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
    
    try:
        # ì²˜ë¦¬ ì‹¤í–‰
        result_df = processor.process_full_pipeline(
            file_pattern="*.dat",
            time_interval='5S',  # 5ì´ˆ ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”)
            save_result=True
        )
        
        if result_df.empty:
            print("âŒ ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*50)
        
        # ê²°ê³¼ ìš”ì•½
        print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ ì‹œê°„ í¬ì¸íŠ¸: {len(result_df):,}ê°œ")
        print(f"   ì´ íŠ¹ì„±(ì»¬ëŸ¼) ìˆ˜: {len(result_df.columns)}ê°œ")
        
        if 'timestamp' in result_df.columns:
            time_range = result_df['timestamp'].max() - result_df['timestamp'].min()
            print(f"   ì‹œê°„ ë²”ìœ„: {time_range}")
            print(f"   ì‹œì‘ ì‹œê°„: {result_df['timestamp'].min()}")
            print(f"   ì¢…ë£Œ ì‹œê°„: {result_df['timestamp'].max()}")
        
        # ì£¼ìš” í†µê³„
        if 'health_score' in result_df.columns:
            avg_health = result_df['health_score'].mean()
            min_health = result_df['health_score'].min()
            max_health = result_df['health_score'].max()
            print(f"   í‰ê·  ê±´ê°•ë„: {avg_health:.1f}% (ë²”ìœ„: {min_health:.1f}% ~ {max_health:.1f}%)")
        
        if 'anomaly_score' in result_df.columns:
            avg_anomaly = result_df['anomaly_score'].mean()
            max_anomaly = result_df['anomaly_score'].max()
            print(f"   í‰ê·  ì´ìƒì ìˆ˜: {avg_anomaly:.3f} (ìµœëŒ€: {max_anomaly:.3f})")
        
        if 'status' in result_df.columns:
            status_counts = result_df['status'].value_counts()
            print(f"   ìƒíƒœ ë¶„í¬:")
            for status, count in status_counts.items():
                percentage = count / len(result_df) * 100
                print(f"     - {status}: {count:,}ê°œ ({percentage:.1f}%)")
        
        # ì €ì¥ëœ íŒŒì¼ ì •ë³´
        csv_files = [f for f in os.listdir(actual_data_dir) if f.startswith('processed_robot_data_') and f.endswith('.csv')]
        if csv_files:
            latest_file = max(csv_files, key=lambda x: os.path.getmtime(os.path.join(actual_data_dir, x)))
            file_path = os.path.join(actual_data_dir, latest_file)
            file_size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"   ì €ì¥ëœ íŒŒì¼: {latest_file}")
            print(f"   íŒŒì¼ í¬ê¸°: {file_size:.1f} MB")
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3í–‰):")
        print(result_df.head(3).to_string())
        
        # ì»¬ëŸ¼ ëª©ë¡
        print(f"\nğŸ“ ìƒì„±ëœ íŠ¹ì„± ëª©ë¡:")
        sensor_cols = [col for col in result_df.columns if any(keyword in col for keyword in ['joint', 'pos', 'velocity', 'torque', 'error'])]
        engineered_cols = [col for col in result_df.columns if any(keyword in col for keyword in ['total_', 'mean_', 'max_', '_ma_', '_std_'])]
        target_cols = [col for col in result_df.columns if col in ['health_score', 'anomaly_score', 'status']]
        
        print(f"   ì„¼ì„œ ë°ì´í„° ({len(sensor_cols)}ê°œ): {sensor_cols[:5]}{'...' if len(sensor_cols) > 5 else ''}")
        print(f"   ì—”ì§€ë‹ˆì–´ë§ íŠ¹ì„± ({len(engineered_cols)}ê°œ): {engineered_cols[:5]}{'...' if len(engineered_cols) > 5 else ''}")
        print(f"   íƒ€ê²Ÿ ë³€ìˆ˜ ({len(target_cols)}ê°œ): {target_cols}")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
        print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"1. AI ëª¨ë¸ í›ˆë ¨:")
        print(f"   from predictive_model import IoTPredictiveMaintenanceModel")
        print(f"   model = IoTPredictiveMaintenanceModel()")
        print(f"   model.train(result_df, epochs=50)")
        print(f"")
        print(f"2. ì €ì¥ëœ CSV íŒŒì¼ ì§ì ‘ ë¡œë“œ:")
        if csv_files:
            print(f"   df = pd.read_csv('{os.path.join(actual_data_dir, latest_file)}')")
        print(f"")
        print(f"3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì— í†µí•©:")
        print(f"   from kafka_streaming import StreamingManager")
        
        # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
        if csv_files:
            return os.path.join(actual_data_dir, latest_file)
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´:")
        import traceback
        traceback.print_exc()
        return None


def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ - ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì²˜ë¦¬"""
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì²˜ë¦¬)")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì°¾ê¸°
    data_dir = "."
    dat_files = [f for f in os.listdir(data_dir) if f.endswith('.dat')]
    
    if not dat_files:
        print("âŒ í˜„ì¬ ë””ë ‰í† ë¦¬ì— .dat íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    first_file = dat_files[0]
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {first_file}")
    
    # ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = XMLDataProcessor(data_dir)
    
    try:
        # ë‹¨ì¼ íŒŒì¼ íŒŒì‹±
        records = processor.parse_xml_file(first_file)
        print(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(records)}ê°œ ë ˆì½”ë“œ")
        
        if records:
            # DataFrame ë³€í™˜
            df = pd.DataFrame(records)
            print(f"ğŸ“Š DataFrame ìƒì„±: {len(df)}í–‰ x {len(df.columns)}ì—´")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
            print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
            print(df.head().to_string())
            
            # íƒœê·¸ëª… í™•ì¸
            unique_tags = df['tag_name'].unique()
            print(f"\nğŸ·ï¸  ë°œê²¬ëœ íƒœê·¸ ({len(unique_tags)}ê°œ):")
            for i, tag in enumerate(unique_tags[:10]):
                print(f"   {i+1}. {tag}")
            if len(unique_tags) > 10:
                print(f"   ... ë° {len(unique_tags)-10}ê°œ ì¶”ê°€")
    
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        quick_test()
    else:
        result_file = main()
        
        if result_file:
            print(f"\nğŸ’¾ ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° íŒŒì¼: {result_file}")
            print("ì´ íŒŒì¼ì„ AI ëª¨ë¸ í›ˆë ¨ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
