"""
ìˆ˜ì •ëœ XML ë°ì´í„° ì²˜ë¦¬ê¸° - ì‹¤ì œ SCARA ë¡œë´‡ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶¤
"""

import os
import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
from typing import Dict, List, Tuple, Optional
import glob
from pathlib import Path
import logging
from collections import defaultdict

# ì•ˆì „í•œ import
try:
    from config import config
except ImportError:
    class Config:
        DATA_DIR = "data"
    config = Config()

try:
    from utils import setup_logging, validate_dataframe, calculate_health_score
except ImportError:
    def setup_logging(name):
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(name)

    def validate_dataframe(df, required_columns):
        return not df.empty and all(col in df.columns for col in required_columns)

    def calculate_health_score(sensors):
        return 100.0

logger = setup_logging(__name__)


class FixedXMLDataProcessor:
    """ì‹¤ì œ SCARA ë¡œë´‡ ë°ì´í„°ì— ë§ì¶˜ XML ì²˜ë¦¬ê¸°"""

    def __init__(self, data_dir: str = None):
        self.data_dir = data_dir or os.path.join(config.DATA_DIR, "xml_data")
        os.makedirs(self.data_dir, exist_ok=True)

        # ì‹¤ì œ ë°ì´í„°ì—ì„œ ë°œê²¬ëœ ì •í™•í•œ íƒœê·¸ ë§¤í•‘
        self.tag_mapping = {
            # Joint 1 (J1)
            '::[scararobot]Ax_J1.ActualPosition': 'joint1_actual_pos',
            '::[scararobot]Ax_J1.PositionCommand': 'joint1_cmd_pos',
            '::[scararobot]Ax_J1.PositionError': 'joint1_pos_error',
            '::[scararobot]Ax_J1.TorqueCommand': 'joint1_torque_cmd',
            '::[scararobot]Ax_J1.TorqueFeedback': 'joint1_torque_feedback',

            # Joint 2 (J2)
            '::[scararobot]Ax_J2.ActualPosition': 'joint2_actual_pos',
            '::[scararobot]Ax_J2.PositionCommand': 'joint2_cmd_pos',
            '::[scararobot]Ax_J2.PositionError': 'joint2_pos_error',
            '::[scararobot]Ax_J2.TorqueCommand': 'joint2_torque_cmd',
            '::[scararobot]Ax_J2.TorqueFeedback': 'joint2_torque_feedback',

            # Joint 3 (J3)
            '::[scararobot]Ax_J3.ActualPosition': 'joint3_actual_pos',
            '::[scararobot]Ax_J3.PositionCommand': 'joint3_cmd_pos',
            '::[scararobot]Ax_J3.PositionError': 'joint3_pos_error',
            '::[scararobot]Ax_J3.TorqueCommand': 'joint3_torque_cmd',
            '::[scararobot]Ax_J3.TorqueFeedback': 'joint3_torque_feedback',

            # Joint 6 (J6)
            '::[scararobot]Ax_J6.ActualPosition': 'joint6_actual_pos',
            '::[scararobot]Ax_J6.PositionCommand': 'joint6_cmd_pos',
            '::[scararobot]Ax_J6.PositionError': 'joint6_pos_error',
            '::[scararobot]Ax_J6.TorqueCommand': 'joint6_torque_cmd',
            '::[scararobot]Ax_J6.TorqueFeedback': 'joint6_torque_feedback',

            # Cartesian ì¢Œí‘œê³„
            '::[scararobot]CS_Cartesian.ActualPosition[0]': 'cartesian_x',
            '::[scararobot]CS_Cartesian.ActualPosition[1]': 'cartesian_y',
            '::[scararobot]CS_Cartesian.ActualPosition[2]': 'cartesian_z',
            '::[scararobot]CS_Cartesian.ActualPosition[3]': 'cartesian_rx',
            '::[scararobot]CS_Cartesian.ActualPosition[4]': 'cartesian_ry',
            '::[scararobot]CS_Cartesian.ActualPosition[5]': 'cartesian_rz',

            # SCARA ì¢Œí‘œê³„
            '::[scararobot]CS_SCARA.ActualPosition[0]': 'scara_x',
            '::[scararobot]CS_SCARA.ActualPosition[1]': 'scara_y',
            '::[scararobot]CS_SCARA.ActualPosition[2]': 'scara_z',
        }

        logger.info(f"ìˆ˜ì •ëœ XML ë°ì´í„° ì²˜ë¦¬ê¸° ì´ˆê¸°í™”: {self.data_dir}")
        logger.info(f"ë§¤í•‘ëœ íƒœê·¸ ìˆ˜: {len(self.tag_mapping)}ê°œ")

    def parse_xml_file(self, file_path: str) -> List[Dict]:
        """ë‹¨ì¼ XML íŒŒì¼ íŒŒì‹± (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            # XML íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # XML íŒŒì‹± (ë£¨íŠ¸ íƒœê·¸ê°€ ì´ë¯¸ ìˆìŒ)
            root = ET.fromstring(content)

            # ë°ì´í„° ì¶”ì¶œ
            records = []
            for historical_data in root.findall('.//HistoricalTextData'):
                try:
                    tag_name = historical_data.find('TagName')
                    status = historical_data.find('Status')
                    tag_value = historical_data.find('TagValue')
                    timestamp = historical_data.find('TimeStamp')

                    if all(elem is not None for elem in [tag_name, status, tag_value, timestamp]):
                        record = {
                            'tag_name': tag_name.text.strip() if tag_name.text else '',
                            'status': int(status.text) if status.text else 0,
                            'tag_value': float(tag_value.text) if tag_value.text else 0.0,
                            'timestamp': timestamp.text.strip() if timestamp.text else '',
                            'file_source': os.path.basename(file_path)
                        }
                        records.append(record)

                except Exception as e:
                    logger.debug(f"ë ˆì½”ë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            logger.debug(f"íŒŒì¼ {os.path.basename(file_path)}: {len(records)}ê°œ ë ˆì½”ë“œ ì¶”ì¶œ")
            return records

        except ET.ParseError as e:
            logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜ ({file_path}): {e}")
            return []
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
            return []

    def process_multiple_files(self, file_pattern: str = "*.dat", max_files: int = None) -> pd.DataFrame:
        """ì—¬ëŸ¬ XML/DAT íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬"""
        file_paths = glob.glob(os.path.join(self.data_dir, file_pattern))

        if not file_paths:
            logger.warning(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {os.path.join(self.data_dir, file_pattern)}")
            return pd.DataFrame()

        # íŒŒì¼ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
        if max_files:
            file_paths = file_paths[:max_files]

        logger.info(f"{len(file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")

        all_records = []
        processed_files = 0

        for file_path in sorted(file_paths):
            try:
                records = self.parse_xml_file(file_path)
                if records:
                    all_records.extend(records)
                    processed_files += 1

                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if processed_files % 50 == 0:
                    logger.info(f"ì§„í–‰ ìƒí™©: {processed_files}/{len(file_paths)} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")

            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
                continue

        if not all_records:
            logger.error("ì²˜ë¦¬ëœ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()

        # DataFrame ìƒì„±
        df = pd.DataFrame(all_records)
        logger.info(f"ì´ {len(df)}ê°œ ë ˆì½”ë“œ ì¶”ì¶œ ì™„ë£Œ")

        return df

    def clean_and_standardize(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° ì •ë¦¬ ë° í‘œì¤€í™” (ìˆ˜ì •ëœ ë²„ì „)"""
        if df.empty:
            return df

        logger.info("ë°ì´í„° ì •ë¦¬ ë° í‘œì¤€í™” ì‹œì‘")
        original_count = len(df)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë¦¬
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        df = df.dropna(subset=['timestamp'])

        # í‘œì¤€ ì„¼ì„œëª…ìœ¼ë¡œ ë³€í™˜
        df['sensor_name'] = df['tag_name'].map(self.tag_mapping)

        # ë§¤í•‘ëœ ì„¼ì„œë§Œ í•„í„°ë§
        mapped_data = df[df['sensor_name'].notna()]

        logger.info(f"ë§¤í•‘ ê²°ê³¼:")
        logger.info(f"  ì›ë³¸ ë ˆì½”ë“œ: {original_count:,}ê°œ")
        logger.info(f"  ê³ ìœ  íƒœê·¸: {df['tag_name'].nunique()}ê°œ")
        logger.info(f"  ë§¤í•‘ëœ íƒœê·¸: {mapped_data['tag_name'].nunique()}ê°œ")
        logger.info(f"  ë§¤í•‘ëœ ë ˆì½”ë“œ: {len(mapped_data):,}ê°œ")

        if mapped_data.empty:
            logger.error("ë§¤í•‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            logger.info("ë°œê²¬ëœ íƒœê·¸ (ìƒìœ„ 10ê°œ):")
            for tag in df['tag_name'].value_counts().head(10).index:
                logger.info(f"  - {tag}")
            return pd.DataFrame()

        # ìƒíƒœê°’ í•„í„°ë§ (ì •ìƒ ë°ì´í„°ë§Œ)
        clean_data = mapped_data[mapped_data['status'] == 0]

        # ì •ë ¬
        clean_data = clean_data.sort_values(['timestamp', 'sensor_name']).reset_index(drop=True)

        logger.info(f"ì •ë¦¬ í›„ {len(clean_data)}ê°œ ë ˆì½”ë“œ")
        return clean_data

    def pivot_to_timeseries(self, df: pd.DataFrame, time_interval: str = '5S') -> pd.DataFrame:
        """ì„¼ì„œ ë°ì´í„°ë¥¼ ì‹œê³„ì—´ í˜•íƒœë¡œ í”¼ë²—"""
        if df.empty:
            return df

        logger.info("ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜")

        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_df = df.pivot_table(
            index='timestamp',
            columns='sensor_name',
            values='tag_value',
            aggfunc='mean'  # ê°™ì€ ì‹œê°„ëŒ€ ì¤‘ë³µ ë°ì´í„°ëŠ” í‰ê· 
        )

        # ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        if time_interval:
            pivot_df = pivot_df.resample(time_interval).mean()

        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì„ í˜• ë³´ê°„)
        pivot_df = pivot_df.interpolate(method='linear', limit_direction='both')

        # ì—¬ì „íˆ ê²°ì¸¡ê°’ì´ ìˆìœ¼ë©´ ì•/ë’¤ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        pivot_df = pivot_df.fillna(method='ffill').fillna(method='bfill')

        # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
        pivot_df = pivot_df.reset_index()

        # ì»¬ëŸ¼ëª… ì •ë¦¬
        pivot_df.columns.name = None

        logger.info(f"ì‹œê³„ì—´ ë³€í™˜ ì™„ë£Œ: {len(pivot_df)}ê°œ ì‹œì , {len(pivot_df.columns)-1}ê°œ ì„¼ì„œ")
        return pivot_df

    def add_engineering_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€ (SCARA ë¡œë´‡ íŠ¹í™”)"""
        if df.empty:
            return df

        logger.info("SCARA ë¡œë´‡ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìˆ˜í–‰")

        # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        if 'timestamp' in df.columns:
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['minute'] = df['timestamp'].dt.minute

        # ê´€ì ˆë³„ ìœ„ì¹˜ ì˜¤ì°¨ íŠ¹ì„±
        joint_error_cols = [col for col in df.columns if 'pos_error' in col]
        if joint_error_cols:
            # ì „ì²´ ìœ„ì¹˜ ì˜¤ì°¨ (RMS)
            df['total_position_error'] = np.sqrt(
                sum(df[col].fillna(0)**2 for col in joint_error_cols) / len(joint_error_cols)
            )

            # í‰ê·  ìœ„ì¹˜ ì˜¤ì°¨
            df['mean_position_error'] = df[joint_error_cols].mean(axis=1)

            # ìµœëŒ€ ìœ„ì¹˜ ì˜¤ì°¨
            df['max_position_error'] = df[joint_error_cols].max(axis=1)

            # ìœ„ì¹˜ ì˜¤ì°¨ í‘œì¤€í¸ì°¨ (ë¶ˆê· í˜• ì§€í‘œ)
            df['position_error_std'] = df[joint_error_cols].std(axis=1)

        # í† í¬ ê´€ë ¨ íŠ¹ì„±
        torque_cmd_cols = [col for col in df.columns if 'torque_cmd' in col]
        torque_fb_cols = [col for col in df.columns if 'torque_feedback' in col]

        if torque_cmd_cols:
            # ì „ì²´ í† í¬ ëª…ë ¹
            df['total_torque_cmd'] = df[torque_cmd_cols].abs().sum(axis=1)

            # í† í¬ ë¶ˆê· í˜•
            df['torque_cmd_imbalance'] = df[torque_cmd_cols].std(axis=1)

            # í† í¬ ë³€í™”ìœ¨
            for col in torque_cmd_cols:
                df[f'{col}_change'] = df[col].diff().abs()

        if torque_fb_cols:
            # í† í¬ í”¼ë“œë°± í•©ê³„
            df['total_torque_feedback'] = df[torque_fb_cols].abs().sum(axis=1)

        # ê´€ì ˆ ì†ë„ ì¶”ì • (ìœ„ì¹˜ ë³€í™”ìœ¨)
        actual_pos_cols = [col for col in df.columns if 'actual_pos' in col and 'joint' in col]
        for col in actual_pos_cols:
            joint_name = col.replace('_actual_pos', '')
            df[f'{joint_name}_velocity_est'] = df[col].diff().abs()

        # Cartesian ì¢Œí‘œ íŠ¹ì„±
        cartesian_cols = [col for col in df.columns if 'cartesian' in col]
        if len(cartesian_cols) >= 3:
            # Cartesian ìœ„ì¹˜ ë²¡í„° í¬ê¸°
            xyz_cols = [col for col in cartesian_cols if any(axis in col for axis in ['_x', '_y', '_z'])]
            if len(xyz_cols) >= 3:
                df['cartesian_distance'] = np.sqrt(
                    sum(df[col].fillna(0)**2 for col in xyz_cols[:3])
                )

        # ë¡¤ë§ í†µê³„ (ì´ë™ í‰ê· , í‘œì¤€í¸ì°¨)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        windows = [5, 10, 20]

        for window in windows:
            for col in numeric_cols:
                if col not in ['hour', 'day_of_week', 'minute']:
                    df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()
                    df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)

        # SCARA ë¡œë´‡ ê±´ê°•ë„ ê³„ì‚°
        df['health_score'] = self._calculate_scara_health(df)

        # ì´ìƒ ì ìˆ˜ ê³„ì‚°
        df['anomaly_score'] = self._calculate_scara_anomaly(df)

        # ìƒíƒœ ë¶„ë¥˜
        df['status'] = df.apply(self._classify_scara_status, axis=1)

        # ë””ë°”ì´ìŠ¤ ID ì¶”ê°€
        df['device_id'] = 'SCARA_ROBOT_001'

        logger.info(f"íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(df.columns)}ê°œ íŠ¹ì„±")
        return df

    def _calculate_scara_health(self, df: pd.DataFrame) -> pd.Series:
        """SCARA ë¡œë´‡ ê±´ê°•ë„ ê³„ì‚°"""
        health_scores = []

        for _, row in df.iterrows():
            score = 100.0

            # ìœ„ì¹˜ ì˜¤ì°¨ í˜ë„í‹° (ê°€ì¥ ì¤‘ìš”)
            if 'total_position_error' in row and not pd.isna(row['total_position_error']):
                error_penalty = min(60, row['total_position_error'] * 20)
                score -= error_penalty

            # í† í¬ ë¶ˆê· í˜• í˜ë„í‹°
            if 'torque_cmd_imbalance' in row and not pd.isna(row['torque_cmd_imbalance']):
                imbalance_penalty = min(25, row['torque_cmd_imbalance'] * 2)
                score -= imbalance_penalty

            # ê³¼ë„í•œ í† í¬ í˜ë„í‹°
            if 'total_torque_cmd' in row and not pd.isna(row['total_torque_cmd']):
                if row['total_torque_cmd'] > 100:  # ì„ê³„ê°’
                    torque_penalty = min(15, (row['total_torque_cmd'] - 100) * 0.1)
                    score -= torque_penalty

            health_scores.append(max(0, score))

        return pd.Series(health_scores)

    def _calculate_scara_anomaly(self, df: pd.DataFrame) -> pd.Series:
        """SCARA ë¡œë´‡ ì´ìƒ ì ìˆ˜ ê³„ì‚°"""
        anomaly_scores = []

        for _, row in df.iterrows():
            score = 0.0

            # ìœ„ì¹˜ ì˜¤ì°¨ ê¸°ë°˜
            if 'total_position_error' in row and not pd.isna(row['total_position_error']):
                score += min(0.6, row['total_position_error'] / 5)

            # ê±´ê°•ë„ ê¸°ë°˜
            if 'health_score' in row:
                score += (100 - row['health_score']) / 150

            # í† í¬ ë¶ˆê· í˜• ê¸°ë°˜
            if 'torque_cmd_imbalance' in row and not pd.isna(row['torque_cmd_imbalance']):
                score += min(0.3, row['torque_cmd_imbalance'] / 30)

            # ìœ„ì¹˜ ì˜¤ì°¨ ë¶ˆê· í˜• ê¸°ë°˜
            if 'position_error_std' in row and not pd.isna(row['position_error_std']):
                score += min(0.1, row['position_error_std'] / 10)

            anomaly_scores.append(min(1.0, score))

        return pd.Series(anomaly_scores)

    def _classify_scara_status(self, row) -> str:
        """SCARA ë¡œë´‡ ìƒíƒœ ë¶„ë¥˜"""
        health_score = row.get('health_score', 100)
        anomaly_score = row.get('anomaly_score', 0)

        if health_score >= 85 and anomaly_score < 0.2:
            return 'normal'
        elif health_score >= 60 and anomaly_score < 0.5:
            return 'warning'
        else:
            return 'critical'

    def save_processed_data(self, df: pd.DataFrame, filename: str = None) -> str:
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        if filename is None:
            filename = f"processed_scara_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        filepath = os.path.join(self.data_dir, filename)
        df.to_csv(filepath, index=False)

        logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {filepath}")
        return filepath

    def process_full_pipeline(self, file_pattern: str = "*.dat",
                            time_interval: str = '5S',
                            max_files: int = None,
                            save_result: bool = True) -> pd.DataFrame:
        """ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("SCARA ë¡œë´‡ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        # 1. XML íŒŒì¼ë“¤ íŒŒì‹±
        df_raw = self.process_multiple_files(file_pattern, max_files)
        if df_raw.empty:
            logger.error("ì›ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()

        # 2. ë°ì´í„° ì •ë¦¬ ë° í‘œì¤€í™”
        df_clean = self.clean_and_standardize(df_raw)
        if df_clean.empty:
            logger.error("ì •ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()

        # 3. ì‹œê³„ì—´ ë³€í™˜
        df_timeseries = self.pivot_to_timeseries(df_clean, time_interval)
        if df_timeseries.empty:
            logger.error("ì‹œê³„ì—´ ë³€í™˜ ì‹¤íŒ¨")
            return pd.DataFrame()

        # 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        df_final = self.add_engineering_features(df_timeseries)

        # 5. ê²°ê³¼ ì €ì¥
        if save_result:
            saved_path = self.save_processed_data(df_final)
            logger.info(f"ìµœì¢… ê²°ê³¼ ì €ì¥: {saved_path}")

        # 6. ìš”ì•½ ì •ë³´
        logger.info("="*60)
        logger.info("ğŸ‰ SCARA ë¡œë´‡ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"  ì›ì‹œ ë ˆì½”ë“œ: {len(df_raw):,}ê°œ")
        logger.info(f"  ì •ë¦¬ í›„: {len(df_clean):,}ê°œ")
        logger.info(f"  ìµœì¢… ì‹œì : {len(df_final):,}ê°œ")
        logger.info(f"  ìµœì¢… íŠ¹ì„±: {len(df_final.columns):,}ê°œ")

        if not df_final.empty:
            logger.info(f"  ì‹œê°„ ë²”ìœ„: {df_final['timestamp'].min()} ~ {df_final['timestamp'].max()}")
            logger.info(f"  í‰ê·  ê±´ê°•ë„: {df_final['health_score'].mean():.1f}%")
            logger.info(f"  í‰ê·  ì´ìƒì ìˆ˜: {df_final['anomaly_score'].mean():.3f}")

            status_counts = df_final['status'].value_counts()
            logger.info(f"  ìƒíƒœ ë¶„í¬: {dict(status_counts)}")

        logger.info("="*60)

        return df_final


def quick_test_fixed_processor():
    """ìˆ˜ì •ëœ í”„ë¡œì„¸ì„œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ìˆ˜ì •ëœ SCARA ë¡œë´‡ ë°ì´í„° ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸")
    print("="*50)

    # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = FixedXMLDataProcessor('.')

    try:
        # ì†Œìˆ˜ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸ (ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ)
        result_df = processor.process_full_pipeline(
            file_pattern="*.dat",
            time_interval='10S',  # 10ì´ˆ ê°„ê²©
            max_files=5,  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ
            save_result=True
        )

        if not result_df.empty:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            print(f"ğŸ“Š ê²°ê³¼: {len(result_df)}ê°œ ì‹œì , {len(result_df.columns)}ê°œ íŠ¹ì„±")

            print(f"\nğŸ“‹ ì£¼ìš” íŠ¹ì„±:")
            feature_categories = {
                'ê´€ì ˆ ìœ„ì¹˜': [col for col in result_df.columns if 'actual_pos' in col],
                'ìœ„ì¹˜ ì˜¤ì°¨': [col for col in result_df.columns if 'pos_error' in col],
                'í† í¬': [col for col in result_df.columns if 'torque' in col],
                'ì—”ì§€ë‹ˆì–´ë§': [col for col in result_df.columns if any(x in col for x in ['total_', 'mean_', 'max_'])],
                'íƒ€ê²Ÿ': [col for col in result_df.columns if col in ['health_score', 'anomaly_score', 'status']]
            }

            for category, cols in feature_categories.items():
                if cols:
                    print(f"  {category}: {len(cols)}ê°œ - {cols[:3]}{'...' if len(cols) > 3 else ''}")

            print(f"\nğŸ“ˆ í†µê³„:")
            print(f"  í‰ê·  ê±´ê°•ë„: {result_df['health_score'].mean():.1f}%")
            print(f"  í‰ê·  ì´ìƒì ìˆ˜: {result_df['anomaly_score'].mean():.3f}")
            print(f"  ìƒíƒœ ë¶„í¬: {dict(result_df['status'].value_counts())}")

        else:
            print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    quick_test_fixed_processor()